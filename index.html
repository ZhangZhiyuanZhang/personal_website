<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhiyuan Zhang</title>

    <meta name="author" content="Zhiyuan Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhiyuan Zhang    张志远
                </p>
                <p style="text-align: center; font-size: 12px;">
                  Email: <a href="mailto:zhan5570@purdue.edu">zhan5570 AT purdue.edu</a>
                </p>
                <p>I'm a first-year PhD student at <a href="https://www.purdue.edu/">Purdue University</a>, advised by Professor <a href="https://www.purduemars.com/">Yu She</a>. 
                  I was honored with the IE Graduate Excellence Award at Purdue University.
                </p>
                <p>
                  I received my Bachelor's and Master's degrees in Mechanical Engineering from <a href="https://english.hust.edu.cn/"> Huazhong University of Science and Technology</a>.
                </p>
                <p style="text-align:center">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AIfU4H6u2NmmVblK6gprjwn1pSlIldzVB8ak_wqUgiDSoKaFkN19cdkjmDtNBV4YxtG45XgsRnUP_bhUAPhuOUwHQFs&user=KWcRRLMAAAAJ">G. Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/ZhiyuanZhang_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ZhangZhiyuanZhang">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/zhiyuan-zhang-046417364/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/photo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <div style="max-height:200px; overflow-y:auto;">
                    <ul>
                      <li>[April 2025] I was honored with the IE Graduate Excellence Award</li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  * indicates equal contribution.
                </p>
                <p>
                  My research aims to design learning algorithms for robotic agents, enabling them to perform everyday manipulation tasks with human-level proficiency. To this end, I am currently focusing on  <strong><span style="color:red;">hierarchical multimodal robot learning</span></strong>.
                  Specifically, my research explores: </p>
                <p>
                  <strong>1. Integrating visual, 3D, and tactile modalities to enable multimodal robot learning. </strong><br>
                  <strong>2. Developing interpretable neural-symbolic low-level policies through differentiable optimization.</strong> <br>
                  <strong>3. Deploying pretrained vision-language models for high-level, open-world reasoning and planning. </strong><br>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

              <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/cp_cover.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">Canonical Policy: Learning Canonical 3D Representation for Equivariant Policy</span>
                  <br>
                  <strong>Zhiyuan Zhang*</strong>, Zhengtong Xu*, Jai Nanda Lakamsani, Yu She
                  <br>
                  <em>Under Review</em>, 2025
                  <br>
                </p>

                <a href="https://zhangzhiyuanzhang.github.io/cp-website/">website</a> /
                <a href="">arXiv(soon)</a> /
                <a href="">video(soon)</a> /
                <a href="">code(soon)</a>
                <p></p>
                <p>
                  Canonical Policy enables equivariant observation-to-action mappings by grouping both in-distribution and out-of-distribution point clouds to a canonical 3D representation.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
              <video playsinline autoplay loop muted src="images/manifeel.mp4" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
              </a></td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning</span>
                  <br>
                    Quan Khanh Luu*, Pokuang Zhou*, Zhengtong Xu*, <strong>Zhiyuan Zhang</strong>, Qiang Qiu, Yu She
                  <br>
                  <em>Under Review</em>, 2025
                  <br>
                </p>

                <a href="">website(soon)</a> /
                <a href="">arXiv(soon)</a> /
                <a href="">video(soon)</a> /
                <a href="">code(soon)</a>
                <p></p>
                <p>
                  ManiFeel is a reproducible and scalable simulation benchmark for studying supervised visuotactile policy learning.
                </p>
              </td>
            </tr>

            <tr>
              <td width="40%" valign="top" align="center">
                <img src="images/gelroller.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
              </td>
              <td width="60%" valign="top">
                <p>
                  <span class="papertitle">GelRoller: A Rolling Vision-based Tactile Sensor for Large Surface Reconstruction Using Self-Supervised Photometric Stereo Method</span>
                  <br>
                  <strong>Zhiyuan Zhang</strong>, Huan Ma, Yulin Zhou, Jingjing Ji, Hua Yang
                  <br>
                  <em>ICRA</em>, 2024
                  <br>
                </p>

                <a href="https://ieeexplore.ieee.org/abstract/document/10610417">paper</a> /
                <a href="https://drive.google.com/file/d/1LbKBnsd991RF80KhjrcS9LOMaMb25xaH/view?usp=drive_link">video</a> /
                <a href="#" onclick="toggleBibtex(event, 'GelRoller'); return false;">bibtex</a>

                <div id="GelRoller" style="display:none; position:absolute; background-color:white; border:1px solid #ccc; padding:10px; width:400px; z-index:100; border-radius:15px; overflow:auto; word-wrap:break-word;">
                  <button onclick="copyBibtex(event, 'GelRoller-content')" style="float:right; margin-bottom:5px;">Copy</button>
                  <pre id="GelRoller-content" style="margin:0;">
                    @INPROCEEDINGS{10610417,
                      author={Zhang, Zhiyuan and Ma, Huan and Zhou, Yulin and Ji, Jingjing and Yang, Hua},
                      booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
                      title={GelRoller: A Rolling Vision-based Tactile Sensor for Large Surface Reconstruction Using Self-Supervised Photometric Stereo Method}, 
                      year={2024},
                      volume={},
                      number={},
                      pages={7961-7967},
                      keywords={Mechanical sensors;Surface reconstruction;Accuracy;Shape;Shape measurement;Tactile sensors;Lighting},
                      doi={10.1109/ICRA57147.2024.10610417}}
                  </pre>
                  <a href="#" onclick="toggleBibtex(event, 'GelRoller'); return false;">Close</a>
                </div>
                
                <p></p>
                <p>
                  By leveraging GelRoller, users can quickly sense large surfaces and reconstruct the 3D shape of the contact region using just a single input image.
                </p>
              </td>
            </tr>

          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Awards</h2>
                  <ul>
                    <li><strong>IE Graduate Excellence Award</strong>, Purdue University, 2025</li>
                    <li><strong>Digital Electronics Scholarship</strong>, Huazhong University of Science and Technology, 2023</li>
                    <li><strong>Xiaomi Scholarship</strong>,  Huazhong University of Science and Technology, 2020</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <script>
      function toggleBibtex(event, bibtexId) {
        var bibtexDiv = document.getElementById(bibtexId);
        if (bibtexDiv.style.display === 'none') {
          var x = event.pageX;
          var y = event.pageY;
          
          bibtexDiv.style.left = x + 'px';
          bibtexDiv.style.top = y + 'px';
          
          bibtexDiv.style.display = 'block';
        } else {
          bibtexDiv.style.display = 'none';
        }
      }

      function copyBibtex(event, contentId) {
        event.stopPropagation();
        const content = document.getElementById(contentId).textContent;
        const lines = content.split('\n')
          .map(line => line.trim())
          .filter(line => line); // Remove empty lines

        const formattedLines = lines.map((line, index) => {
          if (index === 0) return line;  // First line
          if (line === '}') return line;  // Closing brace
          return '  ' + line;  // All other lines
        });

        const formattedContent = formattedLines.join('\n');

        navigator.clipboard.writeText(formattedContent)
          .then(() => {
            const button = event.target;
            const originalText = button.textContent;
            button.textContent = 'Copied!';
            setTimeout(() => {
              button.textContent = originalText;
            }, 1500);
          })
          .catch(err => {
            console.error('Failed to copy:', err);
          });
      }
    </script>
  </body>
</html>
